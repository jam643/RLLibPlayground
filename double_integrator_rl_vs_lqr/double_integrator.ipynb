{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "## Background\n",
    "\n",
    "I want to gain a better intuition about policy gradient based RL algorithms by analyzing it's performance when trained against a very simple environment. Given my controls background, I'm training the RL agent on a simple double-integrator system whose optimal control policy can be provided by LQR. This way, I can compare the RL agent's performance and convergence properties against the known optimal solution. \n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Find control policy for an unconstrained, fully observable, discrete-time double-integrator system with quadratic cost on state and actions.\n",
    "\n",
    "<img src=\"images/double_int.jpeg\" alt=\"Alt text\" width=\"350\"/>\n",
    "\n",
    "### Motion Model\n",
    "$$\n",
    "x_{t+1} \n",
    "=\n",
    "A_d \\cdot x_{t} + B_d \\cdot u_{t}\n",
    "\n",
    "\\\\=\\\\\n",
    "\n",
    "\\begin{bmatrix}\n",
    "s_{t+1} \\\\\n",
    "v_{t+1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & \\Delta t \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_t \\\\\n",
    "v_t\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{2 m} \\Delta t^2 \\\\\n",
    "\\frac{\\Delta t}{m}\n",
    "\\end{bmatrix}\n",
    "u_{t}\n",
    "$$\n",
    "\n",
    "### Cost\n",
    "\n",
    "$$\n",
    "\\min_{u} J = \\sum_{t=0}^{\\infty} (x_t^T \\cdot Q \\cdot x_t + u_t^T \\cdot R \\cdot u_t)\n",
    "$$\n",
    "\n",
    "such that we want to regulate the system to state of zero: $s_{desired}, v_{desired}, a_{desired} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Terminology\n",
    "\n",
    "### Given\n",
    "\n",
    "Some of the added generality with the RL setup include:\n",
    "* May have partially observed state (POMDP)\n",
    "* State dynamics (transition function) can be stochastic/probabilistic\n",
    "* Dynamics and reward functions can be nonlinear/discontinuous and black box (no gradient information)\n",
    "* Rewards can be sparse, e.g. only receiving a reward at the end of the episode.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "\\text{Controls (LQR)} & \\text{Reinforcement Learning} \\\\\n",
    "\\hline\n",
    "\\text{state, action} = x_t, u_t & \\text{state, observation, action} = s_t, o_t, a_t,  \\\\\n",
    "\\text{state is fully observed or estimated} & \\text{observation may not contain full state (POMDP)} \\\\\n",
    "\\hline\n",
    "\\text{cost } c(x_t, u_t) = x_t^T \\cdot Q \\cdot x_t + u_t^T \\cdot R \\cdot u_t & \\text{reward } = r(s_t, a_t) \\\\\n",
    "\\hline\n",
    "\\text{dynamics } x_{t+1} = A_d \\cdot x_{t} + B_d \\cdot u_{t} & \\text{Transition function } s_{t+1} \\sim P(s_{t+1} \\mid s_t, a_t) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Objective\n",
    "\n",
    "RL objective is to find a policy that maximizes the expected value of the sum of discounted rewards. The expected value $\\mathbb{E}$ accounts for stochastic nature of transition and policy functions.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "\\text{Controls (LQR)} & \\text{Reinforcement Learning} \\\\\n",
    "\\hline\n",
    "\\pi^* = \\arg\\min_{\\pi} J = \\sum_{t=0}^{\\infty} (x_t^T \\cdot Q \\cdot x_t + u_t^T \\cdot R \\cdot u_t) &  \\pi^* = \\arg\\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t r(s_t, a_t) \\right]  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Result\n",
    "\n",
    "With LQR the optimal policy is a linear function of the current state and the cost-to-go is a quadratic function of the current state.\n",
    "\n",
    "With policy gradient based Deep RL approaches, the actions are sampled from a stochastic policy, represented by a neural net, that provides a distribution of actions given the current state. Often time the RL algorithm also captures the the value function $V(s)$ representing the expected discounted reward-to-go. \n",
    "\n",
    "In both cases, the cost/reward-to-go ends up being a useful tool given that both approaches utilize Bellman's Principle of optimality and Dynamic Programming methods.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c}\n",
    "\\text{Controls (LQR)} & \\text{Reinforcement Learning} \\\\\n",
    "\\hline\n",
    "u_t = -K \\cdot x_t &  a_t \\sim \\pi_{\\theta}(a_t \\mid s_t)  \\\\\n",
    "\\hline\n",
    "\\text{cost-to-go: } J(x) = x^T \\cdot P \\cdot x & \\text{value fn: } V(s) = V^{\\pi}_{\\theta}(s) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete LQR Solution\n",
    "\n",
    "Solve for P in this discrete-time Algebraic Riccati Equation:\n",
    "\n",
    "$$\n",
    "P = A^T P A - A^T P B \\left( R + B^T P B \\right)^{-1} B^T P A + Q\n",
    "$$\n",
    "\n",
    "The optimal control linear feedback gains can then be computed as:\n",
    "\n",
    "$$\n",
    "K = \\left( R + B^T P B \\right)^{-1} B^T P A\n",
    "$$\n",
    "\n",
    "Such that the optimal policy is:\n",
    "\n",
    "$$\n",
    "u_k = -K x_k\n",
    "$$\n",
    "\n",
    "And the corresponding cost-to-go is:\n",
    "\n",
    "$$\n",
    "J_k = x_k^T P x_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning (Policy Gradient)\n",
    "\n",
    "## Visual Explanation\n",
    "\n",
    "Policy gradient deep RL methods represent policy as deep neural network:\n",
    "\n",
    "<img src=\"images/deep_rl.png\" alt=\"Alt text\" width=\"500\"/>\n",
    "\n",
    "At a high level, training the policy involves:\n",
    "* Sampling trajectories from the environment under the current policy\n",
    "* Estimate (discounted) reward-to-go for actions taken given how the episode plays out\n",
    "* Update the policy to make actions that lead to good rewards more likely and actions that lead to bad rewards less likely:\n",
    "  * Compute policy gradient (gradient of expected reward-to-go w.r.t. policy's neural network's parameters)\n",
    "  * Take small step in direction of policy gradient\n",
    "\n",
    "<img src=\"images/policy_grad_visualized.png\" alt=\"Alt text\" width=\"500\"/>\n",
    "\n",
    "<img src=\"images/rl_training_loop.png\" alt=\"Alt text\" width=\"400\"/>\n",
    "\n",
    "## Landscape of RL algorithms\n",
    "\n",
    "<img src=\"images/rl_policies.png\" alt=\"Alt text\" width=\"700\"/>\n",
    "\n",
    "## Pseudocode for PPO Algorithm\n",
    "\n",
    "1. **Initialize:**\n",
    "    - Environment `Env`, policy `π(θ)`, value function `V(φ)`\n",
    "    - Set learning rates `α_θ`, `α_φ`, discount factor `γ`, clipping parameter `ε`, GAE parameter `λ`\n",
    "    \n",
    "2. **Training loop:**\n",
    "    - **for** each iteration in `range(num_iterations)`:\n",
    "        - **Collect trajectories:**\n",
    "            - **for** each actor in `range(num_actors)`:\n",
    "                - `state = Env.reset()`, `trajectory = []`\n",
    "                - **for** each step in `range(trajectory_length)`:\n",
    "                    - `action = π(state | θ)`\n",
    "                    - `next_state, reward, done, _ = Env.step(action)`\n",
    "                    - Append `(state, action, reward, done)` to `trajectory`\n",
    "                    - Update `state`, **break** if `done`\n",
    "                - Append `trajectory` to `trajectories`\n",
    "        \n",
    "        - **Compute returns and advantages:**\n",
    "            - **for** each `trajectory` in `trajectories`:\n",
    "                - Calculate `returns` and `advantages` using GAE:\n",
    "                    - Initialize `R = 0`\n",
    "                    - **for** `t` in reversed `range(len(rewards))`:\n",
    "                        - `R = rewards[t] + γ * R`\n",
    "                        - `returns[t] = R`, `advantages[t] = R - V(states[t] | φ)`\n",
    "            - Normalize `advantages`\n",
    "        \n",
    "        - **Update policy and value function:**\n",
    "            - **for** _ in `range(policy_updates)`:\n",
    "                - Compute `ratio = π(action | θ) / π_old(action | θ_old)`\n",
    "                - Compute clipped loss: `policy_loss = -mean(min(ratio * advantages, clipped_adv))`\n",
    "                - Update `θ = θ - α_θ * ∇policy_loss`\n",
    "                - Compute value loss: `value_loss = mse(V(state | φ), returns)`\n",
    "                - Update `φ = φ - α_φ * ∇value_loss`\n",
    "        \n",
    "        - Optionally update `θ_old = θ` and log metrics\n",
    "    \n",
    "3. **Return trained policy:** `return π(θ)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RL Agent with Stable-Baselines3\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html\n",
    "\n",
    "## Defining a Custom Env\n",
    "\n",
    "Stable-Baselines3 provides implementations of a variety of RL algorithms. Thus the remaining engineering challenge becomes implementing an \"environment\" class that the policy can interact with.\n",
    "\n",
    "See `double_integrator_env.py` to see the implementation of the double integrator environment.\n",
    "\n",
    "\n",
    "That environment follows the basic structure of creating any custom environment, which entails deriving from `gym.Env` abstract class and implementing the abstract methods. E.g.:\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super().__init__()\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # Example when using discrete actions:\n",
    "        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        ...\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        ...\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        ...\n",
    "\n",
    "    def close(self):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RL Policy via PPO Against the Double Integrator Env\n",
    "\n",
    "Training of the agent is done in `train_model.py`. The following is a minimal example of training against the custom `DoubleIntegratorEnv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x36d302d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from double_integrator_env import DoubleIntegratorEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    DoubleIntegratorEnv,\n",
    "    n_envs=1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    device=device,\n",
    "    gamma=1.0,  # no discounting to make comparable with LQR\n",
    ")\n",
    "\n",
    "# Train the policy\n",
    "timesteps = 10000\n",
    "model.learn(\n",
    "        total_timesteps=timesteps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model that trained for 150000 time steps (~20hrs of sim training which takes a couple minutes running on machine)\n",
    "model_name = \"PPO_double_integrator\"\n",
    "model_steps = \"150000\"\n",
    "\n",
    "vec_env = make_vec_env(DoubleIntegratorEnv, n_envs=1)\n",
    "\n",
    "model_path = f\"models/{model_name}/{model_steps}\"\n",
    "model = PPO.load(model_path, env=vec_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard can be used to visualize metrics during the training process, to check for convergence:\n",
    "\n",
    "`tensorboard --logdir double_integrator_rl_vs_lqr`\n",
    "\n",
    "<img src=\"images/tensorboard_evals.png\" alt=\"Alt text\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model\n",
    "\n",
    "The following shows minimalist example, see `eval_policy.py` for more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init state: State(station=5.0, speed=0.0), Final state: State(station=-0.105164014, speed=4.749745e-07)\n"
     ]
    }
   ],
   "source": [
    "from double_integrator_env import DoubleIntegratorEnv, State\n",
    "env = DoubleIntegratorEnv()\n",
    "\n",
    "obs, _ = env.reset(init_state=State(station = 5.0, speed=0.0))\n",
    "obs_array = [obs]\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, returns, terminated, truncated, info = env.step(action)\n",
    "    obs_array.append(obs)\n",
    "    # env.render()\n",
    "    done = terminated or truncated\n",
    "    \n",
    "print(f\"Init state: {State(*obs_array[0])}, Final state: {State(*obs_array[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Env and Evaluating Model Performance\n",
    "\n",
    "Comparison of RL model performance (blue) with LQR (orange), against a handful of simulated episodes with varying initial station. Left figure shows poor performance after 10,000 training steps, vs close-to-optimal (LQR) performance after 150,000 training steps.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"eval_images/PPO_double_integrator/10000.png\" alt=\"Image 1\" style=\"width:100%;\"/></td>\n",
    "        <td><img src=\"eval_images/PPO_double_integrator/150000.png\" alt=\"Image 2\" style=\"width:100%;\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspecting the Policy\n",
    "\n",
    "In this example I'm using the default policy network architecture with 2 hidden layers containing 64 neurons each. Below shows examples of introspecting into the policy. [See stable-baselines3 docs here](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html) for more info on specifying custom model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (pi_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (vf_features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer: Linear(in_features=2, out_features=64, bias=True)\n",
      "first layer weights: Parameter containing:\n",
      "tensor([[ 0.2057,  0.2936],\n",
      "        [ 0.2202, -0.0627],\n",
      "        [ 0.1767,  0.0925],\n",
      "        [ 0.1223, -0.2022],\n",
      "        [ 0.0190,  0.0274],\n",
      "        [-0.2214, -0.0778],\n",
      "        [ 0.3491,  0.3136],\n",
      "        [ 0.1179, -0.2065],\n",
      "        [-0.0693,  0.0929],\n",
      "        [ 0.1432,  0.3056],\n",
      "        [ 0.0834,  0.0274],\n",
      "        [-0.2687,  0.3104],\n",
      "        [-0.0620, -0.2876],\n",
      "        [-0.1799, -0.1159],\n",
      "        [ 0.0402, -0.3521],\n",
      "        [ 0.0879,  0.0530],\n",
      "        [-0.1200, -0.0903],\n",
      "        [ 0.1208, -0.0016],\n",
      "        [-0.0260, -0.1442],\n",
      "        [ 0.0093, -0.2957],\n",
      "        [-0.3042,  0.0740],\n",
      "        [-0.1716,  0.1210],\n",
      "        [-0.2364, -0.0104],\n",
      "        [-0.2699,  0.3688],\n",
      "        [-0.0184,  0.0177],\n",
      "        [ 0.2390,  0.0218],\n",
      "        [ 0.2032, -0.1881],\n",
      "        [-0.1471,  0.2591],\n",
      "        [ 0.0304, -0.2351],\n",
      "        [ 0.0508,  0.1211],\n",
      "        [ 0.0874, -0.0269],\n",
      "        [-0.0573,  0.0612],\n",
      "        [ 0.0520, -0.0719],\n",
      "        [-0.3298, -0.0959],\n",
      "        [ 0.2292,  0.3869],\n",
      "        [-0.0639,  0.0278],\n",
      "        [-0.1503, -0.1571],\n",
      "        [ 0.0810,  0.0324],\n",
      "        [-0.2667, -0.1495],\n",
      "        [-0.0541,  0.1016],\n",
      "        [-0.3026,  0.0769],\n",
      "        [-0.1580, -0.0286],\n",
      "        [ 0.2020,  0.0043],\n",
      "        [-0.1206,  0.0817],\n",
      "        [ 0.2916,  0.1365],\n",
      "        [-0.0719, -0.2905],\n",
      "        [-0.2122, -0.0713],\n",
      "        [-0.1862,  0.1617],\n",
      "        [-0.1315, -0.1840],\n",
      "        [-0.0290, -0.0396],\n",
      "        [ 0.0266, -0.4140],\n",
      "        [-0.1139, -0.1310],\n",
      "        [-0.0825,  0.2737],\n",
      "        [ 0.1355,  0.1896],\n",
      "        [-0.1383, -0.0304],\n",
      "        [ 0.0041, -0.0488],\n",
      "        [ 0.0072,  0.0272],\n",
      "        [-0.0635, -0.0652],\n",
      "        [ 0.4333, -0.1886],\n",
      "        [ 0.0270, -0.1814],\n",
      "        [-0.0770,  0.1132],\n",
      "        [-0.3665, -0.0337],\n",
      "        [ 0.2878, -0.1941],\n",
      "        [ 0.0762, -0.0923]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Access the first layer of the policy network\n",
    "first_layer = model.policy.mlp_extractor.policy_net[0]\n",
    "print(f\"First layer: {first_layer}\")\n",
    "\n",
    "# Print the weights of the first layer\n",
    "print(f\"first layer weights: {first_layer.weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             192\n",
      "              Tanh-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              Tanh-4                   [-1, 64]               0\n",
      "================================================================\n",
      "Total params: 4,352\n",
      "Trainable params: 4,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Print a summary of the policy network\n",
    "print(summary(model.policy.mlp_extractor.policy_net, input_size=(model.observation_space.shape[0],)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the state\n",
    "state = np.array([0.0, 0.0])\n",
    "\n",
    "# Convert the state to a tensor\n",
    "state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "# Get the action distribution from the policy\n",
    "with torch.no_grad():\n",
    "    action_dist = model.policy(state_tensor)\n",
    "\n",
    "# Extract mean and standard deviation for the Gaussian distribution\n",
    "mean_action = action_dist.distribution.mean.cpu().numpy().flatten()\n",
    "std_action = action_dist.distribution.variance.sqrt().cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Value Function Network\n",
    "\n",
    "The trained PPO policy contains both a learned policy network and value network estimated the reward-to-go. This reward-to-go estimate can also be compared to the LQR's value function (which is a quadratic function of state) as another indicator of whether the model is generalizing/converging close to the optimal solution. The figure below, generated by `eval_value_fn.png` shows that the value function network closely approximates the LQR's optimal value function.\n",
    "\n",
    "<img src=\"images/value_fn_comparison.png\" alt=\"Alt text\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlplayground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
